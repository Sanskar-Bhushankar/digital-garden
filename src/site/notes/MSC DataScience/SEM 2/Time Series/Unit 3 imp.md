---
{"dg-publish":true,"permalink":"/MSC DataScience/SEM 2/Time Series/Unit 3 imp/","created":"2025-06-25T14:03:46.404+05:30"}
---


# Unit 3 theory

### Q1. Define forecasting. What are the main goals and applications of forecasting in business and economics?

**Forecasting:**
Forecasting is the process of predicting future events or trends based on historical data, analysis, and models.
### **Main Goals of Forecasting:**
1Ô∏è‚É£ **Future Planning:**  
To estimate future demand, sales, or production needs.
2Ô∏è‚É£ **Risk Reduction:**  
Helps to reduce uncertainty by providing data-based insights.
3Ô∏è‚É£ **Better Decision-Making:**  
Assists managers and policymakers in making strategic choices.
4Ô∏è‚É£ **Resource Allocation:**  
Helps in proper budgeting and resource management.
5Ô∏è‚É£ **Performance Improvement:**  
Businesses can track and improve future performance by learning from trends.

**Applications in Business and Economics:**
* **Sales forecasting** to plan production and inventory
* **Financial forecasting** for budgeting and investment decisions
* **Economic forecasting** for GDP, inflation, and employment trends
* **Supply chain management** for demand planning
* **Marketing strategy** and product launches based on trend predictions

---

### Q2. Differentiate between qualitative and quantitative forecasting methods. Give two examples of each.
#### **Qualitative Forecasting Methods**

**What is it?**
These methods are based on **human judgment, opinions, and intuition**, not on historical data. They are used when **past data is not available** or when forecasting new products, trends, or situations.
**When to use?**
* When historical data is missing or unreliable
* When launching a new product or entering a new market
* When expert opinion is more useful than past patterns
**Examples:**
1. **Delphi Method** ‚Äì A panel of experts is asked to give their opinions anonymously. Their responses are collected, summarized, and shared in rounds until a consensus forecast is reached.
2. **Market Research** ‚Äì Surveys or interviews with customers to predict demand or trends based on what people say they will do or prefer.
#### **Quantitative Forecasting Methods**
**What is it?**
These methods use **mathematical models and historical data** to make forecasts. They rely on data patterns, trends, seasonality, and statistics.
**When to use?**
* When you have enough reliable historical data
* When patterns like trends or seasonality exist
* When you want data-driven, objective results
**Examples:**
1. **Moving Average** ‚Äì Takes the average of previous values (like sales over last 3 months) to predict the next value.
2. **Exponential Smoothing** ‚Äì Assigns more weight to recent data and less to older data to forecast future values more accurately.
##### Summary:


| **Basis**         | **Qualitative Forecasting**                                   | **Quantitative Forecasting**                          |
| ----------------- | ------------------------------------------------------------- | ----------------------------------------------------- |
| **Definition**    | Based on **expert opinions, intuition, or judgment**.         | Based on **mathematical models and historical data**. |
| **Data Required** | Does **not require numerical data**.                          | Requires **past numerical data** for calculations.    |
| **Usefulness**    | Useful when **data is not available** or future is uncertain. | Useful when **past trends can be analyzed**.          |
| **Accuracy**      | **Less accurate**; subjective.                                | **More accurate**; objective and data-driven.         |
| **Examples**      | 1. Delphi Method                                              | 1. Time Series Analysis  <br>2. Regression Analysis   |




### **Conclusion:**

Qualitative methods rely on **opinions and experience**, while quantitative methods use **mathematical models and data** for forecasting.

---
---
### Q3. Explain the variate component method in time series analysis. What are its key components?

##### **What is Variate Component Method?**
It is a method in time series analysis where we **break the time series data into parts** (called components) so we can **understand and analyze each part separately**.

The **Variate Component Method** is a technique used in **time series analysis** to **break down** (decompose) a time series into its **individual components** to better understand patterns and make predictions.

---
### **Key Components of Time Series:**

1Ô∏è‚É£ **Trend (T):**  
It shows the **long-term movement** or direction of the data over time.  
Slow and steady increase or decrease in data over a long time.
Example: Gradual increase in sales over years.

2Ô∏è‚É£ **Seasonal Component (S):**  
It refers to **regular patterns** that repeat at fixed periods (like months or quarters).  
Patterns that repeat regularly in a fixed time (like every year or month)
Example: Increase in ice-cream sales during summer.

3Ô∏è‚É£ **Cyclical Component (C):**  
It refers to **long-term up and down movements** not of fixed period, usually related to the business cycle.  
Ups and downs in data over a long time but not in a fixed pattern.
Example: Economic booms or recessions.

4Ô∏è‚É£ **Irregular/Random Component (I):**  
It includes Unexpected or random changes in the data
Example: Sudden drop in sales due to natural disaster.

### **Models Used:**
- **Additive Model:**This model expresses a time series as the sum of its trend, seasonal, cyclical, and irregular components.
    
    Y=T+S+C+I
- **Multiplicative Model:** **:** This model expresses a time series as the product of its trend, seasonal, cyclical, and irregular components, often used when variations change proportionally with the series magnitude.
    
    Y=T√óS√óC√óI
    
    (Used when variations increase or decrease with time)
    

---

### **Conclusion:**

The **Variate Component Method** helps to **analyze and forecast** time series by studying each component **separately**, improving understanding of the data behavior.

---

### Q4. When would you choose an additive model over a multiplicative model in time series decomposition? Explain with examples.

##### **Use Additive Model when:**
* The **change** (up or down) is always the **same amount**, no matter how big or small the overall values are.
* Use when **seasonal and irregular variations are constant** over time.
**Example:**
Every December, a shop sells **100 extra books** for Christmas ‚Äî always **+100**, not more or less depending on the year.
So, use **Additive** model:
**Sales = Trend + Seasonal + Irregular + Cyclical** 
##### **Use Multiplicative Model when:**
* The **change** is a **percentage** of the total ‚Äî it **grows or shrinks** as the trend grows.
* Use when **seasonal or irregular variations increase or decrease** with the trend.
**Example:**
A shop sees a **20% increase** in summer sales every year.
If normal sales are 1000, summer sales = 1200.
If normal sales grow to 2000, summer sales = 2400.
So, use **Multiplicative** model:
**Sales = Trend √ó Seasonal √ó Irregular x Cyclical**
##### **In short:**
* **Additive = fixed amount change**
* **Multiplicative = percentage change**

---
### Q5. What is a stationary time series? Distinguish between strict stationarity and weak stationarity.
**Stationary Time Series (Simple Explanation):**

A **stationary time series** is one where the data‚Äôs key properties **do not change over time** ‚Äî specifically:
* **Mean (average)** stays constant
* **Variance (spread of data)** stays constant
* **Covariance** (relationship between values at different times) depends only on the time gap, not the actual time
##### **Types of Stationarity:**
##### **1. Strict Stationarity:**
**Strict Stationarity**) means that the **entire distribution** of a time series remains the **same over time** ‚Äî not just the mean or variance, but **all moments and joint distributions**.
* The **entire distribution** (not just mean/variance) stays the same over time.
* All statistical properties are unchanged if we shift time.
**Example:** If the data pattern looks the same whether you look at day 1‚Äì10 or day 100‚Äì110, it is strictly stationary.
![SEM 2/Time Series/attachments/Pasted image 20250625101211.png](/img/user/MSC%20DataScience/SEM%202/Time%20Series/attachments/Pasted%20image%2020250625101211.png)
##### **2. Weak Stationarity (also called Covariance Stationarity):**
**Weak Stationarity** (also called **Second-order Stationarity**) means that a time series has **constant statistical properties** over time ‚Äî specifically **mean**, **variance**, and **covariance**.
* Only the **mean, variance, and covariance** are constant over time.
* This is more commonly used in practice.
**Example:** A series with constant average sales and similar ups/downs over time is weakly stationary.
![SEM 2/Time Series/attachments/Pasted image 20250625100926.png](/img/user/MSC%20DataScience/SEM%202/Time%20Series/attachments/Pasted%20image%2020250625100926.png)
##### **Key Differences:**

| Feature           | Strict Stationarity           | Weak Stationarity                   |
| ----------------- | ----------------------------- | ----------------------------------- |
| Based on          | Full distribution             | Mean, variance, and covariance only |
| Strict conditions | Yes                           | Relaxed conditions                  |
| Usage in practice | Rarely checked (hard to test) | Often used in models like ARIMA     |

### Q6. Define autocorrelation function (ACF). How does it help in identifying the structure of a time series?
### ‚úÖ **Definition:**

The **Autocorrelation Function (ACF)** measures the **correlation** between a time series and its **past values (lags)**.  
It tells us how much the present value of the series is related to its past values.

> In simple words:  
> ACF shows how current data points are **similar or related** to previous ones over different time gaps (lags).

---

### ‚úÖ **Formula:**

![SEM 2/Time Series/attachments/Pasted image 20250625115158.png](/img/user/MSC%20DataScience/SEM%202/Time%20Series/attachments/Pasted%20image%2020250625115158.png)
---

## **How ACF Helps in Time Series Analysis**

### üîç **1. Identifying Time Series Patterns:**
- Helps detect **trend**, **seasonality**, or **randomness**.
- High autocorrelation at certain lags indicates **repeating patterns**.

### üß± **2. Model Selection:**
- ACF plot (called **Correlogram**) helps in choosing models like:
    - **AR (Autoregressive)** ‚Üí if ACF decreases gradually.
    - **MA (Moving Average)** ‚Üí if ACF cuts off after a few lags.
    - **ARMA/ARIMA** ‚Üí combined behavior.
        
### üìâ **3. Checking Stationarity:**
- If ACF drops to zero quickly, the series is likely **stationary**.
- If ACF stays high ‚Üí **non-stationary** (needs differencing).

---

### ‚úÖ **Conclusion:**

The **Autocorrelation Function** is a key tool in time series analysis. It helps to **understand structure, choose models**, and test if data is **stationary** or not.

### Q7. What is a correlogram? How is it useful in time series analysis?
**Definition:**
A **correlogram** is a **graphical representation of the autocorrelation function (ACF)** of a time series. It shows how correlated the current value is with its past values (lags), plotted against the number of lags.
##### **How it is Useful in Time Series Analysis:**
1. **Detects Serial Correlation:**
   Helps identify if data points are related to previous values.

2. **Checks Stationarity:**
   If autocorrelations drop quickly (cut off), the series may be stationary. If they decay slowly, the series is likely non-stationary.

3. **Identifies Seasonality:**
   Regular repeating spikes at specific lags suggest seasonal patterns.

4. **Model Selection:**
   Helps decide the order of AR (AutoRegressive) or MA (Moving Average) components for ARIMA models.

5. **Visual Diagnostic Tool:**
   Makes it easier to understand autocorrelation behavior instead of reading values from a table.
##### **Example:**
A correlogram showing strong spikes at lag 12, 24, etc., may indicate **yearly seasonality in monthly sales data**.

---

> [!NOTE]
> simple exponential smoothing:  
> a method to forecast when data has no trend or seasonality. it gives more weight to recent values and less to older ones. used when data stays mostly stable over time.
> 
> holt‚Äôs double exponential smoothing:  
> an improved version that works when data has a trend. it smooths both the level and the trend to give better forecasts. used when values are steadily increasing or decreasing.
Here is a **simple, exam-ready answer** for **5 marks** on **Brown‚Äôs Discounted Regression**:

---

## Brown‚Äôs Discounted Regression

### ‚úÖ **Definition:**

**Brown‚Äôs Discounted Regression** is a forecasting method that gives **more weight to recent data** and **less weight to older data** using a **discounting factor**.

It is often used when **recent trends are more important** than older patterns, especially in **changing environments**.

---

### ‚úÖ **Key Idea:**

![SEM 2/Time Series/attachments/Pasted image 20250625123304.png](/img/user/MSC%20DataScience/SEM%202/Time%20Series/attachments/Pasted%20image%2020250625123304.png).

---

### ‚úÖ **Formula (Conceptual):**

![SEM 2/Time Series/attachments/Pasted image 20250625123315.png](/img/user/MSC%20DataScience/SEM%202/Time%20Series/attachments/Pasted%20image%2020250625123315.png)

---

### ‚úÖ **Uses:**

- Short-term forecasting in **dynamic environments**.
- Helpful when data has **trending behavior** or **recent changes** are important.
    

---

### ‚úÖ **Advantages:**

- Quick to update forecasts with new data.
    
- Gives more importance to **recent trends**.
    
- Suitable for **real-time decision-making**.
    

---

### ‚ùå **Disadvantages:**

- Not suitable for data with **seasonality**.
    
- Choosing the **right discount factor** is critical.
    
- Less effective if past data is equally important.
    

---

### ‚úÖ **Conclusion:**

**Brown‚Äôs Discounted Regression** is a practical forecasting method that adjusts predictions by **discounting older data**, making it ideal for **fast-changing time series**.

---


### Q8. Compare simple exponential smoothing and Holt‚Äôs double exponential smoothing. When is each method appropriate?
**Comparison: Simple Exponential Smoothing vs. Holt‚Äôs Double Exponential Smoothing**

| Feature               | Simple Exponential Smoothing (SES)             | Holt‚Äôs Double Exponential Smoothing            |
| --------------------- | ---------------------------------------------- | ---------------------------------------------- |
| **Trend Handling**    | Does **not** handle trend                      | Handles **linear trend**                       |
| **Components Used**   | Only **level**                                 | **Level + Trend**                              |
| **Complexity**        | Simple to apply                                | More complex (uses two smoothing constants)    |
| **Forecast Equation** | Based on weighted average of past values       | Adds a trend term to the forecast              |
| **When to Use**       | For **stationary data** (no trend/seasonality) | For **data with a trend** (but no seasonality) |

##### **When is Each Method Appropriate?**
* **Simple Exponential Smoothing (SES):**
  Use when the data has **no clear trend or seasonality** ‚Äî e.g., stable monthly demand for a basic item.
* **Holt‚Äôs Double Exponential Smoothing:**
  Use when the data shows a **consistent upward or downward trend** ‚Äî e.g., steadily increasing product sales over time.
##### **Example:**
* **SES:** Daily electricity usage in a factory with stable demand.
* **Holt‚Äôs:** Monthly revenue of a growing company.

---
### Q9. Discuss any two exponential smoothing methods to forecast the future data.

##### **1. simple exponential smoothing:**
this method is used when the data has no trend or seasonality. it gives more weight to recent values and less to older ones using a smoothing factor called alpha (Œ±), which lies between 0 and 1.
the formula is:
forecast = Œ± √ó current actual value + (1 ‚àí Œ±) √ó previous forecast
if Œ± is close to 1, more weight is given to the recent data.
this method is easy to apply and best suited for short-term forecasts where data is steady without much change.
**example:**
forecasting daily demand for bread in a shop where sales are usually around 100 units with small variations.
**advantages:**
* easy to compute
* works well for stable time series
* requires less data
**limitations:**
* does not work if there is trend or seasonality in data
##### **2. holt‚Äôs double exponential smoothing:**
this is an extension of simple exponential smoothing and is used when the data has a trend (either upward or downward). it uses two equations ‚Äî one for the current level and one for the trend.
it uses two smoothing constants:
* alpha (Œ±) for the level
* beta (Œ≤) for the trend
the forecast is adjusted by both the current level and the trend value. it is more accurate than simple exponential smoothing when there is a clear trend.
**example:**
forecasting monthly sales of a product that increases by 20 units every month.
**advantages:**
* captures both level and trend
* better accuracy for trending data
* suitable for short to medium-term forecasting
**limitations:**
* does not handle seasonality
* needs careful tuning of Œ± and Œ≤ values

---

# Unit 3 - Numerical

## Q.![SEM 2/Time Series/attachments/Pasted image 20250625113426.png](/img/user/MSC%20DataScience/SEM%202/Time%20Series/attachments/Pasted%20image%2020250625113426.png)

# Autocorrelation Calculation for Time Series

## Given Data

Observations: 34, 24, 23, 31, 38, 34, 35, 31, 29, 28 n = 10 observations

## Step 1: Calculate the Mean

xÃÑ = (34 + 24 + 23 + 31 + 38 + 34 + 35 + 31 + 29 + 28) √∑ 10 xÃÑ = 307 √∑ 10 = **30.7**

## Step 2: Calculate Deviations from Mean

|t|xt|xt - xÃÑ|(xt - xÃÑ)¬≤|
|---|---|---|---|
|1|34|3.3|10.89|
|2|24|-6.7|44.89|
|3|23|-7.7|59.29|
|4|31|0.3|0.09|
|5|38|7.3|53.29|
|6|34|3.3|10.89|
|7|35|4.3|18.49|
|8|31|0.3|0.09|
|9|29|-1.7|2.89|
|10|28|-2.7|7.29|

**Sum of squared deviations = 208.1**

## Step 3: Calculate Sample Variance

s¬≤ = Œ£(xt - xÃÑ)¬≤ √∑ (n-1) = 208.1 √∑ 9 = **23.12**

## Step 4: Calculate Autocorrelations

### For r‚ÇÅ (lag 1):

Products: (xt - xÃÑ)(xt+1 - xÃÑ) for t = 1 to 9

|t|(xt - xÃÑ)|(xt+1 - xÃÑ)|Product|
|---|---|---|---|
|1|3.3|-6.7|-22.11|
|2|-6.7|-7.7|51.59|
|3|-7.7|0.3|-2.31|
|4|0.3|7.3|2.19|
|5|7.3|3.3|24.09|
|6|3.3|4.3|14.19|
|7|4.3|0.3|1.29|
|8|0.3|-1.7|-0.51|
|9|-1.7|-2.7|4.59|

Sum = 72.01 **r‚ÇÅ = 72.01 √∑ (9 √ó 23.12) = 0.346**

### For r‚ÇÇ (lag 2):

Products: (xt - xÃÑ)(xt+2 - xÃÑ) for t = 1 to 8

|t|(xt - xÃÑ)|(xt+2 - xÃÑ)|Product|
|---|---|---|---|
|1|3.3|-7.7|-25.41|
|2|-6.7|0.3|-2.01|
|3|-7.7|7.3|-56.21|
|4|0.3|3.3|0.99|
|5|7.3|4.3|31.39|
|6|3.3|0.3|0.99|
|7|4.3|-1.7|-7.31|
|8|0.3|-2.7|-0.81|

Sum = -58.38 **r‚ÇÇ = -58.38 √∑ (8 √ó 23.12) = -0.316**

### For r‚ÇÉ (lag 3):

Products: (xt - xÃÑ)(xt+3 - xÃÑ) for t = 1 to 7

|t|(xt - xÃÑ)|(xt+3 - xÃÑ)|Product|
|---|---|---|---|
|1|3.3|0.3|0.99|
|2|-6.7|7.3|-48.91|
|3|-7.7|3.3|-25.41|
|4|0.3|4.3|1.29|
|5|7.3|0.3|2.19|
|6|3.3|-1.7|-5.61|
|7|4.3|-2.7|-11.61|

Sum = -87.07 **r‚ÇÉ = -87.07 √∑ (7 √ó 23.12) = -0.538**

### For r‚ÇÑ (lag 4):

Products: (xt - xÃÑ)(xt+4 - xÃÑ) for t = 1 to 6

|t|(xt - xÃÑ)|(xt+4 - xÃÑ)|Product|
|---|---|---|---|
|1|3.3|7.3|24.09|
|2|-6.7|3.3|-22.11|
|3|-7.7|4.3|-33.11|
|4|0.3|0.3|0.09|
|5|7.3|-1.7|-12.41|
|6|3.3|-2.7|-8.91|

Sum = -52.36 **r‚ÇÑ = -52.36 √∑ (6 √ó 23.12) = -0.377**

### For r‚ÇÖ (lag 5):

Products: (xt - xÃÑ)(xt+5 - xÃÑ) for t = 1 to 5

|t|(xt - xÃÑ)|(xt+5 - xÃÑ)|Product|
|---|---|---|---|
|1|3.3|3.3|10.89|
|2|-6.7|4.3|-28.81|
|3|-7.7|0.3|-2.31|
|4|0.3|-1.7|-0.51|
|5|7.3|-2.7|-19.71|

Sum = -40.45 **r‚ÇÖ = -40.45 √∑ (5 √ó 23.12) = -0.350**

## Final Results:

- **r‚ÇÅ = 0.346**
- **r‚ÇÇ = -0.316**
- **r‚ÇÉ = -0.538**
- **r‚ÇÑ = -0.377**
- **r‚ÇÖ = -0.350**

## Formula Used:

**r‚Çñ = Œ£(xt - xÃÑ)(xt+k - xÃÑ) √∑ [(n-k) √ó s¬≤]**

where:

- k = lag
- n = number of observations
- s¬≤ = sample variance

![SEM 2/Time Series/attachments/Pasted image 20250625113552.png](/img/user/MSC%20DataScience/SEM%202/Time%20Series/attachments/Pasted%20image%2020250625113552.png)
![SEM 2/Time Series/attachments/Pasted image 20250625113612.png](/img/user/MSC%20DataScience/SEM%202/Time%20Series/attachments/Pasted%20image%2020250625113612.png)
### Q1. Given a time series Y = [5, 6, 4, 7, 6, 5, 4, 6, 5, 6], compute the mean and variance. Verify if the mean and variance are constant.

## Given Data

Time Series Y = [5, 6, 4, 7, 6, 5, 4, 6, 5, 6] n = 10 observations

---

## Step 1: Calculate Overall Mean and Variance

### Mean Calculation Table

|t|Y‚Çú|
|---|---|
|1|5|
|2|6|
|3|4|
|4|7|
|5|6|
|6|5|
|7|4|
|8|6|
|9|5|
|10|6|
|**Total**|**54**|

**Overall Mean (»≤) = 54 √∑ 10 = 5.4**

### Variance Calculation Table

|t|Y‚Çú|(Y‚Çú - »≤)|(Y‚Çú - »≤)¬≤|
|---|---|---|---|
|1|5|-0.4|0.16|
|2|6|0.6|0.36|
|3|4|-1.4|1.96|
|4|7|1.6|2.56|
|5|6|0.6|0.36|
|6|5|-0.4|0.16|
|7|4|-1.4|1.96|
|8|6|0.6|0.36|
|9|5|-0.4|0.16|
|10|6|0.6|0.36|
|**Total**|**54**|**0**|**8.40**|

**Variance (œÉ¬≤) = Œ£(Y‚Çú - »≤)¬≤ √∑ n = 8.40 √∑ 10 = 0.84**

---

## Step 2: Test for Constant Mean (Stationarity in Mean)

### Method: Split Series into Two Halves

**First Half (t = 1 to 5):** Y‚ÇÅ = [5, 6, 4, 7, 6] **Second Half (t = 6 to 10):** Y‚ÇÇ = [5, 4, 6, 5, 6]

### Mean Comparison Table

| Period             | Observations      | Sum    | Mean    |
| ------------------ | ----------------- | ------ | ------- |
| First Half (1-5)   | 5, 6, 4, 7, 6     | 28     | **5.6** |
| Second Half (6-10) | 5, 4, 6, 5, 6     | 26     | **5.2** |
| **Overall**        | **All 10 values** | **54** | **5.4** |
|                    |                   |        |         |

**Difference in Means = |5.6 - 5.2| = 0.4**

---

## Step 3: Test for Constant Variance (Stationarity in Variance)

### Variance of Each Half

**First Half Variance:**

|t|YÔøΩt|(Y‚Çú - »≤‚ÇÅ)|(Y‚Çú - »≤‚ÇÅ)¬≤|
|---|---|---|---|
|1|5|-0.6|0.36|
|2|6|0.4|0.16|
|3|4|-1.6|2.56|
|4|7|1.4|1.96|
|5|6|0.4|0.16|
|**Total**|**28**|**0**|**5.20**|

**œÉ‚ÇÅ¬≤ = 5.20 √∑ 5 = 1.04**

**Second Half Variance:**

|t|Y‚Çú|(Y‚Çú - »≤‚ÇÇ)|(Y‚Çú - »≤‚ÇÇ)¬≤|
|---|---|---|---|
|6|5|-0.2|0.04|
|7|4|-1.2|1.44|
|8|6|0.8|0.64|
|9|5|-0.2|0.04|
|10|6|0.8|0.64|
|**Total**|**26**|**0**|**2.80**|

**œÉ‚ÇÇ¬≤ = 2.80 √∑ 5 = 0.56**

### Variance Comparison Table

| Period             | Variance |
| ------------------ | -------- |
| First Half (1-5)   | **1.04** |
| Second Half (6-10) | **0.56** |
| **Overall**        | **0.84** |
|                    |          |

**Difference in Variances = |1.04 - 0.56| = 0.48**

---

## Step 4: Results and Conclusion

### Summary Table

| Parameter    | Overall | First Half | Second Half | Difference |
| ------------ | ------- | ---------- | ----------- | ---------- |
| **Mean**     | 5.4     | 5.6        | 5.2         | 0.4        |
| **Variance** | 0.84    | 1.04       | 0.56        | 0.48       |
|              |         |            |             |            |

### **Conclusion:**

1. **Mean Constancy:** The difference between first and second half means is 0.4, which is relatively small (7.4% of overall mean). The mean appears **approximately constant**.
    
2. **Variance Constancy:** The difference between first and second half variances is 0.48, which is significant (57% difference). The variance shows **notable variation** between periods.
    
3. **Overall Assessment:** The time series shows reasonable stability in mean but **significant variation in variance**, indicating it may not be perfectly stationary.
    

---

## **Final Answer:**

- **Overall Mean = 5.4**
- **Overall Variance = 0.84**
- **Mean is approximately constant** (small variation of 0.4)
- **Variance is NOT constant** (significant variation of 0.48)
---
### Q2. For the series Y = [5, 6, 4, 7, 6, 5, 4, 6, 5, 6], compute autocovariance and autocorrelation at lag 1.

## Given Data

Yields: 47, 64, 23, 71, 38, 64, 55, 41, 59, 48 n = 10 observations

---

## Step 1: Calculate Mean

### Mean Calculation Table

|t|Y‚Çú|
|---|---|
|1|47|
|2|64|
|3|23|
|4|71|
|5|38|
|6|64|
|7|55|
|8|41|
|9|59|
|10|48|
|**Total**|**510**|

**Mean (»≤) = 510 √∑ 10 = 51**

---

## Step 2: Calculate Deviations and Squared Deviations

### Deviation Calculation Table

|t|Y‚Çú|(Y‚Çú - »≤)|(Y‚Çú - »≤)¬≤|
|---|---|---|---|
|1|47|-4|16|
|2|64|13|169|
|3|23|-28|784|
|4|71|20|400|
|5|38|-13|169|
|6|64|13|169|
|7|55|4|16|
|8|41|-10|100|
|9|59|8|64|
|10|48|-3|9|
|**Total**|**510**|**0**|**1896**|

---

## Step 3: Calculate Autocovariance at Lag 0

**C‚ÇÄ = Œ£(Y‚Çú - »≤)¬≤ √∑ n = 1896 √∑ 10 = 189.6**

---

## Step 4: Calculate Autocovariance at Lag 1 (C‚ÇÅ)

### Lag 1 Autocovariance Table

For C‚ÇÅ, we need (Y‚Çú - »≤)(Y‚Çú‚Çä‚ÇÅ - »≤) for t = 1 to 9

|t|Y‚Çú|Y‚Çú‚Çä‚ÇÅ|(Y‚Çú - »≤)|(Y‚Çú‚Çä‚ÇÅ - »≤)|(YÔøΩt - »≤)(Y‚Çú‚Çä‚ÇÅ - »≤)|
|---|---|---|---|---|---|
|1|47|64|-4|13|-52|
|2|64|23|13|-28|-364|
|3|23|71|-28|20|-560|
|4|71|38|20|-13|-260|
|5|38|64|-13|13|-169|
|6|64|55|13|4|52|
|7|55|41|4|-10|-40|
|8|41|59|-10|8|-80|
|9|59|48|8|-3|-24|
|**Total**|||||**-1497**|

**C‚ÇÅ = Œ£(Y‚Çú - »≤)(Y‚Çú‚Çä‚ÇÅ - »≤) √∑ (n-1) = -1497 √∑ 9 = -166.33**

---

## Step 5: Calculate Autocovariance at Lag 2 (C‚ÇÇ)

### Lag 2 Autocovariance Table

For C‚ÇÇ, we need (Y‚Çú - »≤)(Y‚Çú‚Çä‚ÇÇ - »≤) for t = 1 to 8

|t|Y‚Çú|Y‚Çú‚Çä‚ÇÇ|(Y‚Çú - »≤)|(Y‚Çú‚Çä‚ÇÇ - »≤)|(Y‚Çú - »≤)(Y‚Çú‚Çä‚ÇÇ - »≤)|
|---|---|---|---|---|---|
|1|47|23|-4|-28|112|
|2|64|71|13|20|260|
|3|23|38|-28|-13|364|
|4|71|64|20|13|260|
|5|38|55|-13|4|-52|
|6|64|41|13|-10|-130|
|7|55|59|4|8|32|
|8|41|48|-10|-3|30|
|**Total**|||||**876**|

**C‚ÇÇ = Œ£(Y‚Çú - »≤)(Y‚Çú‚Çä‚ÇÇ - »≤) √∑ (n-2) = 876 √∑ 8 = 109.5**

---

## Step 6: Calculate Autocorrelation Coefficients

### Autocorrelation Formula

**œÅ‚Çñ = C‚Çñ √∑ C‚ÇÄ**

### Autocorrelation Calculation Table

|Lag (k)|Autocovariance (C‚Çñ)|Autocorrelation (œÅ‚Çñ)|
|---|---|---|
|0|189.6|1.000|
|1|-166.33|-0.877|
|2|109.5|0.578|

**Calculations:**

- œÅ‚ÇÄ = C‚ÇÄ √∑ C‚ÇÄ = 189.6 √∑ 189.6 = **1.000**
- œÅ‚ÇÅ = C‚ÇÅ √∑ C‚ÇÄ = -166.33 √∑ 189.6 = **-0.877**
- œÅ‚ÇÇ = C‚ÇÇ √∑ C‚ÇÄ = 109.5 √∑ 189.6 = **0.578**

---

## Step 7: Summary of Results

### Final Results Table

|Parameter|Value|
|---|---|
|**Mean (»≤)**|**51**|
|**Autocovariance at lag 0 (C‚ÇÄ)**|**189.6**|
|**Autocovariance at lag 1 (C‚ÇÅ)**|**-166.33**|
|**Autocovariance at lag 2 (C‚ÇÇ)**|**109.5**|
|**Autocorrelation at lag 0 (œÅ‚ÇÄ)**|**1.000**|
|**Autocorrelation at lag 1 (œÅ‚ÇÅ)**|**-0.877**|
|**Autocorrelation at lag 2 (œÅ‚ÇÇ)**|**0.578**|

---

## **Key Formulas Used:**

1. **Mean:** »≤ = Œ£Y‚Çú √∑ n
    
2. **Autocovariance:** C‚Çñ = Œ£(Y‚Çú - »≤)(Y‚Çú‚Çä‚Çñ - »≤) √∑ (n-k)
    
3. **Autocorrelation:** œÅ‚Çñ = C‚Çñ √∑ C‚ÇÄ
    

---

## **Interpretation:**

- **Strong negative correlation** at lag 1 (œÅ‚ÇÅ = -0.877) indicates consecutive values tend to be opposite in nature
- **Moderate positive correlation** at lag 2 (œÅ‚ÇÇ = 0.578) suggests some cyclical pattern
- The process shows **significant serial correlation**, indicating the yields are not independent
### Q3. Apply simple exponential smoothing with Œ± = 0.5 to the series Y = [50, 52, 53, 54, 56]. The initial forecast is 50. Provide forecast for t = 6.
we are given:

* series Y = \[50, 52, 53, 54, 56]
* alpha (Œ±) = 0.5
* initial forecast (F‚ÇÅ) = 50
we will apply **simple exponential smoothing** using the formula:
**F‚Çú = Œ± √ó Y‚Çú‚Çã‚ÇÅ + (1 ‚àí Œ±) √ó F‚Çú‚Çã‚ÇÅ**
goal: find forecast for t = 6 (i.e., forecast next value after 56)
#### step-by-step calculations:

**F‚ÇÅ = 50 (given)**

**t = 2:**
F‚ÇÇ = 0.5 √ó 50 + 0.5 √ó 50 = 25 + 25 = **50**

**t = 3:**
F‚ÇÉ = 0.5 √ó 52 + 0.5 √ó 50 = 26 + 25 = **51**

**t = 4:**
F‚ÇÑ = 0.5 √ó 53 + 0.5 √ó 51 = 26.5 + 25.5 = **52**

**t = 5:**
F‚ÇÖ = 0.5 √ó 54 + 0.5 √ó 52 = 27 + 26 = **53**

**t = 6 (forecast):**
F‚ÇÜ = 0.5 √ó 56 + 0.5 √ó 53 = 28 + 26.5 = **54.5**
#### final answer:

**forecast for t = 6 is 54.5**

---
### Q4. Using Holt‚Äôs double exponential smoothing, compute the forecast for t = 4 given Y = [100, 108, 117], Œ± = 0.6, Œ≤ = 0.4.

we are given:
* time series Y = \[100, 108, 117]
* smoothing constants: Œ± = 0.6, Œ≤ = 0.4
* need to forecast for **t = 4** using **holt‚Äôs double exponential smoothing**

holt's method uses two equations:
* **level (L‚Çú):** L‚Çú = Œ± √ó Y‚Çú + (1 ‚àí Œ±) √ó (L‚Çú‚Çã‚ÇÅ + T‚Çú‚Çã‚ÇÅ)
* **trend (T‚Çú):** T‚Çú = Œ≤ √ó (L‚Çú ‚àí L‚Çú‚Çã‚ÇÅ) + (1 ‚àí Œ≤) √ó T‚Çú‚Çã‚ÇÅ
* **forecast:** F‚Çú‚Çä‚ÇÅ = L‚Çú + T‚Çú
#### step 1: initialize (t = 1)
L‚ÇÅ = Y‚ÇÅ = 100
T‚ÇÅ = Y‚ÇÇ ‚àí Y‚ÇÅ = 108 ‚àí 100 = 8
#### step 2: t = 2
Y‚ÇÇ = 108
L‚ÇÇ = Œ± √ó Y‚ÇÇ + (1 ‚àí Œ±) √ó (L‚ÇÅ + T‚ÇÅ)
L‚ÇÇ = 0.6 √ó 108 + 0.4 √ó (100 + 8) = 64.8 + 0.4 √ó 108 = 64.8 + 43.2 = **108**

T‚ÇÇ = Œ≤ √ó (L‚ÇÇ ‚àí L‚ÇÅ) + (1 ‚àí Œ≤) √ó T‚ÇÅ
T‚ÇÇ = 0.4 √ó (108 ‚àí 100) + 0.6 √ó 8 = 0.4 √ó 8 + 4.8 = 3.2 + 4.8 = **8**
#### step 3: t = 3
Y‚ÇÉ = 117
L‚ÇÉ = 0.6 √ó 117 + 0.4 √ó (108 + 8) = 70.2 + 0.4 √ó 116 = 70.2 + 46.4 = **116.6**
T‚ÇÉ = 0.4 √ó (116.6 ‚àí 108) + 0.6 √ó 8 = 0.4 √ó 8.6 + 4.8 = 3.44 + 4.8 = **8.24**
#### step 4: forecast for t = 4
F‚ÇÑ = L‚ÇÉ + T‚ÇÉ = 116.6 + 8.24 = **124.84**
**final answer: forecast for t = 4 is 124.84**

---
### Q5. Given quarterly data Y = [30, 21, 29, 36, 42, 33, 41, 48], apply additive Holt-Winters method to compute L5, T5, S5 and forecast for t = 9. Use Œ± = 0.5, Œ≤ = 0.4, Œ≥ = 0.3, s = 4.
### Holt-Winters Additive Method Solution
#### Given Data
- Y = [30, 21, 29, 36, 42, 33, 41, 48] (8 quarters of data)
- Œ± = 0.5 (level smoothing parameter)
- Œ≤ = 0.4 (trend smoothing parameter)  
- Œ≥ = 0.3 (seasonal smoothing parameter)
- s = 4 (seasonal period - quarterly data)
- Find: L‚ÇÖ, T‚ÇÖ, S‚ÇÖ and forecast for t = 9

## Step 1: Initialize Parameters
### Initial Level (L‚ÇÅ)
For additive model, we use the average of first season:
L‚ÇÅ = (Y‚ÇÅ + Y‚ÇÇ + Y‚ÇÉ + Y‚ÇÑ)/4 = (30 + 21 + 29 + 36)/4 = 116/4 = 29
**Why:** The initial level represents the deseasonalized average of the first complete seasonal cycle.
### Initial Trend (T‚ÇÅ)
T‚ÇÅ = [(Y‚ÇÖ + Y‚ÇÜ + Y‚Çá + Y‚Çà) - (Y‚ÇÅ + Y‚ÇÇ + Y‚ÇÉ + Y‚ÇÑ)]/(2s)
T‚ÇÅ = [(42 + 33 + 41 + 48) - (30 + 21 + 29 + 36)]/(2√ó4)
T‚ÇÅ = [164 - 116]/8 = 48/8 = 6

**Why:** The initial trend is calculated as the difference between the averages of two consecutive seasons, divided by 2s to get the trend per period.
### Initial Seasonal Indices (S‚ÇÅ, S‚ÇÇ, S‚ÇÉ, S‚ÇÑ)
For additive model: S·µ¢ = Y·µ¢ - L‚ÇÅ
- S‚ÇÅ = Y‚ÇÅ - L‚ÇÅ = 30 - 29 = 1
- S‚ÇÇ = Y‚ÇÇ - L‚ÇÅ = 21 - 29 = -8  
- S‚ÇÉ = Y‚ÇÉ - L‚ÇÅ = 29 - 29 = 0
- S‚ÇÑ = Y‚ÇÑ - L‚ÇÅ = 36 - 29 = 7
**Why:** Initial seasonal indices represent the deviation of each period from the base level in the first season.
## Step 2: Apply Holt-Winters Equations
The three equations for additive model are:
- Level: L‚Çú = Œ±(Y‚Çú - S‚Çú‚Çã‚Çõ) + (1-Œ±)(L‚Çú‚Çã‚ÇÅ + T‚Çú‚Çã‚ÇÅ)
- Trend: T‚Çú = Œ≤(L‚Çú - L‚Çú‚Çã‚ÇÅ) + (1-Œ≤)T‚Çú‚Çã‚ÇÅ  
- Seasonal: S‚Çú = Œ≥(Y‚Çú - L‚Çú) + (1-Œ≥)S‚Çú‚Çã‚Çõ
### For t = 2:
**Level:** L‚ÇÇ = Œ±(Y‚ÇÇ - S‚ÇÇ‚Çã‚ÇÑ) + (1-Œ±)(L‚ÇÅ + T‚ÇÅ)
Since we don't have S‚Çã‚ÇÇ, we use S‚ÇÇ = -8
L‚ÇÇ = 0.5(21 - (-8)) + 0.5(29 + 6) = 0.5(29) + 0.5(35) = 14.5 + 17.5 = 32
**Trend:** T‚ÇÇ = Œ≤(L‚ÇÇ - L‚ÇÅ) + (1-Œ≤)T‚ÇÅ = 0.4(32 - 29) + 0.6(6) = 0.4(3) + 3.6 = 4.8
**Seasonal:** S‚ÇÇ = Œ≥(Y‚ÇÇ - L‚ÇÇ) + (1-Œ≥)S‚ÇÇ = 0.3(21 - 32) + 0.7(-8) = 0.3(-11) + (-5.6) = -8.9
### For t = 3:
**Level:** L‚ÇÉ = 0.5(29 - 0) + 0.5(32 + 4.8) = 14.5 + 18.4 = 32.9
**Trend:** T‚ÇÉ = 0.4(32.9 - 32) + 0.6(4.8) = 0.36 + 2.88 = 3.24
**Seasonal:** S‚ÇÉ = 0.3(29 - 32.9) + 0.7(0) = 0.3(-3.9) = -1.17
### For t = 4:
**Level:** L‚ÇÑ = 0.5(36 - 7) + 0.5(32.9 + 3.24) = 14.5 + 18.07 = 32.57
**Trend:** T‚ÇÑ = 0.4(32.57 - 32.9) + 0.6(3.24) = -0.132 + 1.944 = 1.812
**Seasonal:** S‚ÇÑ = 0.3(36 - 32.57) + 0.7(7) = 1.029 + 4.9 = 5.929
### For t = 5:
**Level:** L‚ÇÖ = 0.5(42 - 1) + 0.5(32.57 + 1.812) = 20.5 + 17.191 = 37.691
**Trend:** T‚ÇÖ = 0.4(37.691 - 32.57) + 0.6(1.812) = 2.0484 + 1.0872 = 3.1356
**Seasonal:** S‚ÇÖ = 0.3(42 - 37.691) + 0.7(1) = 1.2927 + 0.7 = 1.9927
### Step 3: Results for t = 5
- **L‚ÇÖ = 37.691** (Level at period 5)
- **T‚ÇÖ = 3.136** (Trend at period 5)  
- **S‚ÇÖ = 1.993** (Seasonal index at period 5)
### Step 4: Forecast for t = 9
For additive Holt-Winters, the forecast equation is:
≈∂‚Çú‚Çä‚Çï = L‚Çú + h√óT‚Çú + S‚Çú‚Çä‚Çú‚Çã‚Çõ
Where:
- t = 8 (last observed period)
- h = 1 (one period ahead forecast)
- We need L‚Çà, T‚Çà, and S‚Çâ‚Çã‚ÇÑ = S‚ÇÖ
First, let's continue calculations to find L‚Çà and T‚Çà:
### For t = 6:
L‚ÇÜ = 0.5(33 - (-8.9)) + 0.5(37.691 + 3.136) = 20.95 + 20.4135 = 41.3635
T‚ÇÜ = 0.4(41.3635 - 37.691) + 0.6(3.136) = 1.469 + 1.8816 = 3.3506
### For t = 7:
L‚Çá = 0.5(41 - (-1.17)) + 0.5(41.3635 + 3.3506) = 21.085 + 22.357 = 43.442
T‚Çá = 0.4(43.442 - 41.3635) + 0.6(3.3506) = 0.8314 + 2.0104 = 2.8418
### For t = 8:
L‚Çà = 0.5(48 - 5.929) + 0.5(43.442 + 2.8418) = 21.0355 + 23.142 = 44.1775
T‚Çà = 0.4(44.1775 - 43.442) + 0.6(2.8418) = 0.294 + 1.7051 = 1.9991
### Forecast for t = 9:
≈∂‚Çâ = L‚Çà + 1√óT‚Çà + S‚ÇÖ
≈∂‚Çâ = 44.1775 + 1.9991 + 1.993 = **48.17**
### Final Answer
- L‚ÇÖ = 37.69
- T‚ÇÖ = 3.14  
- S‚ÇÖ = 1.99
- Forecast for t = 9: **48.17**

---

### Q6. Apply Brown‚Äôs discounted regression (double exponential smoothing) with Œ± = 0.5 to Y = [50, 52, 53, 54]. Compute level, trend, and forecast for t = 5.

 **Problem:**
Apply **Brown‚Äôs double exponential smoothing (a.k.a. discounted regression)**
Given:
* Time series: **Y = \[50, 52, 53, 54]**
* Smoothing constant: **Œ± = 0.5**
* Goal: Compute **Level**, **Trend**, and **Forecast** for **t = 5**

#### Step 1: What is Brown‚Äôs Double Exponential Smoothing?
This method is used when the **time series has a trend**, but **no seasonality**.
It smooths the data twice to estimate:
* The **Level** (baseline value)
* The **Trend** (upward/downward direction)
* Then uses both to **forecast future values**
We use two smoothed series:
* **S‚ÇÅ(t)** = single exponential smoothing
* **S‚ÇÇ(t)** = double exponential smoothing

#### Step 2: Formulas Used
These are the core formulas:
4. **S‚ÇÅ(t) = Œ± √ó Y‚Çú + (1 ‚àí Œ±) √ó S‚ÇÅ(t‚àí1)**
   ‚Üí smooths the raw data once
5. **S‚ÇÇ(t) = Œ± √ó S‚ÇÅ(t) + (1 ‚àí Œ±) √ó S‚ÇÇ(t‚àí1)**
   ‚Üí smooths the smoothed data (S‚ÇÅ) again
6. **Level (a‚Çú) = 2 √ó S‚ÇÅ(t) ‚àí S‚ÇÇ(t)**
   ‚Üí estimates current value based on double smoothing
7. **Trend (b‚Çú) = (Œ± / (1 ‚àí Œ±)) √ó (S‚ÇÅ(t) ‚àí S‚ÇÇ(t))**
   ‚Üí measures the slope of the data (rate of change)
8. **Forecast (F‚Çú‚Çä‚Çï) = a‚Çú + b‚Çú √ó h**
   ‚Üí future forecast h steps ahead

#### Step 3: Initialization (Why?)
At t = 1, we need starting values.
We start by setting:
* S‚ÇÅ(1) = Y‚ÇÅ = **50**
* S‚ÇÇ(1) = Y‚ÇÅ = **50**
  This is standard for Brown‚Äôs method (use first value as base).
#### Step 4: Apply the Smoothing Equations (Why?)
Now apply formulas from t = 2 to t = 4 to calculate S‚ÇÅ and S‚ÇÇ.
#### At t = 2 (Y‚ÇÇ = 52):
**S‚ÇÅ(2)** = 0.5 √ó 52 + 0.5 √ó 50 = **51**
**‚Üí Why?** Smooth the actual value (52) with the previous smoothed value (50)

**S‚ÇÇ(2)** = 0.5 √ó 51 + 0.5 √ó 50 = **50.5**
**‚Üí Why?** Smooth S‚ÇÅ(2) using S‚ÇÇ(1)
#### At t = 3 (Y‚ÇÉ = 53):
**S‚ÇÅ(3)** = 0.5 √ó 53 + 0.5 √ó 51 = **52**
**S‚ÇÇ(3)** = 0.5 √ó 52 + 0.5 √ó 50.5 = **51.25**
#### At t = 4 (Y‚ÇÑ = 54):
**S‚ÇÅ(4)** = 0.5 √ó 54 + 0.5 √ó 52 = **53**
**S‚ÇÇ(4)** = 0.5 √ó 53 + 0.5 √ó 51.25 = **52.125**

#### Step 5: Calculate Level and Trend at t = 4 (Why?)
Now that we‚Äôve smoothed the data, we estimate:
#### **Level (a‚ÇÑ):**
a‚ÇÑ = 2 √ó S‚ÇÅ(4) ‚àí S‚ÇÇ(4) = 2 √ó 53 ‚àí 52.125 = **53.875**
**‚Üí Why?** This estimates the base value by adjusting for double smoothing.

#### **Trend (b‚ÇÑ):**
b‚ÇÑ = (Œ± / (1 ‚àí Œ±)) √ó (S‚ÇÅ(4) ‚àí S‚ÇÇ(4))
\= (0.5 / 0.5) √ó (53 ‚àí 52.125) = 1 √ó 0.875 = **0.875**
**‚Üí Why?** Measures how much the series is rising each step.
### Step 6: Forecast for t = 5 (Why?)
We use:
**F‚ÇÖ = a‚ÇÑ + b‚ÇÑ √ó 1 = 53.875 + 0.875 = 54.75**
**‚Üí Why?** We're projecting one time step into the future using the current level + one unit of the trend.
#### Final Answer:
* **Level at t = 4 = 53.875**
* **Trend at t = 4 = 0.875**
* **Forecast for t = 5 = 54.75**
---
### Q7. Given AR(1) model Yt = 0.8Yt‚àí1 + œµt , with Y0 = 10 and œµt = [0.5, ‚àí0.3, 1.0, ‚àí0.2, 0.1], compute Y1 to Y5.

### AR(1) Model Step-by-Step Solution
### Given Information
- **AR(1) Model:** Y‚Çú = 0.8Y‚Çú‚Çã‚ÇÅ + Œµ‚Çú
- **Initial Value:** Y‚ÇÄ = 10
- **Error Terms:** Œµ‚Çú = [0.5, -0.3, 1.0, -0.2, 0.1] for t = 1, 2, 3, 4, 5
- **Task:** Compute Y‚ÇÅ to Y‚ÇÖ
### Understanding the AR(1) Model
### What is AR(1)?
**AR(1)** stands for **Autoregressive model of order 1**, which means:
- **Auto**: The variable depends on itself
- **Regressive**: It's a regression on past values
- **Order 1**: It depends on only 1 previous time period
### Model Components Explained:
- **Y‚Çú**: Current value we want to find
- **0.8**: Autoregressive coefficient (œÜ‚ÇÅ) - shows how much current value depends on previous value
- **Y‚Çú‚Çã‚ÇÅ**: Previous period's value
- **Œµ‚Çú**: Random error/shock term for current period
### Why This Formula Works:
The equation Y‚Çú = 0.8Y‚Çú‚Çã‚ÇÅ + Œµ‚Çú tells us:
9. **80% of current value** comes from the previous value (0.8Y‚Çú‚Çã‚ÇÅ)
10. **20% is new information** from the error term (Œµ‚Çú)
11. Since 0.8 < 1, the series will gradually return to zero if no shocks occur
## Step-by-Step Calculations
### Step 1: Calculate Y‚ÇÅ
**Given:** Y‚ÇÄ = 10, Œµ‚ÇÅ = 0.5
**Formula:** Y‚ÇÅ = 0.8Y‚ÇÄ + Œµ‚ÇÅ **Calculation:** Y‚ÇÅ = 0.8 √ó 10 + 0.5 = 8.0 + 0.5 = **8.5**
**Why this step:** We start with the initial condition Y‚ÇÄ = 10 and apply the AR(1) formula to get the first forecasted value. The 0.8 coefficient means we take 80% of the previous value, then add the random shock.
### Step 2: Calculate Y‚ÇÇ
**Given:** Y‚ÇÅ = 8.5, Œµ‚ÇÇ = -0.3
**Formula:** Y‚ÇÇ = 0.8Y‚ÇÅ + Œµ‚ÇÇ **Calculation:** Y‚ÇÇ = 0.8 √ó 8.5 + (-0.3) = 6.8 - 0.3 = **6.5**
**Why this step:** Now Y‚ÇÅ becomes our "previous value" and we apply the same logic. The negative error term (-0.3) pushes the value down from what it would have been with just the autoregressive component.
### Step 3: Calculate Y‚ÇÉ
**Given:** Y‚ÇÇ = 6.5, Œµ‚ÇÉ = 1.0
**Formula:** Y‚ÇÉ = 0.8Y‚ÇÇ + Œµ‚ÇÉ **Calculation:** Y‚ÇÉ = 0.8 √ó 6.5 + 1.0 = 5.2 + 1.0 = **6.2**
**Why this step:** The positive shock (1.0) is relatively large, boosting the value significantly above what the autoregressive component alone would give us (5.2).
### Step 4: Calculate Y‚ÇÑ
**Given:** Y‚ÇÉ = 6.2, Œµ‚ÇÑ = -0.2
**Formula:** Y‚ÇÑ = 0.8Y‚ÇÉ + Œµ‚ÇÑ **Calculation:** Y‚ÇÑ = 0.8 √ó 6.2 + (-0.2) = 4.96 - 0.2 = **4.76**
**Why this step:** The negative shock (-0.2) pulls the value down slightly from the autoregressive component (4.96).

### Step 5: Calculate Y‚ÇÖ
**Given:** Y‚ÇÑ = 4.76, Œµ‚ÇÖ = 0.1
**Formula:** Y‚ÇÖ = 0.8Y‚ÇÑ + Œµ‚ÇÖ **Calculation:** Y‚ÇÖ = 0.8 √ó 4.76 + 0.1 = 3.808 + 0.1 = **3.908**
**Why this step:** The small positive shock (0.1) slightly increases the value from the autoregressive component.
#### Summary of Results

|Time (t)|Previous Value (Y‚Çë‚Çã‚ÇÅ)|Error (Œµ‚Çú)|AR Component (0.8Y‚Çú‚Çã‚ÇÅ)|Final Value (Y‚Çú)|
|---|---|---|---|---|
|1|10.000|0.5|8.000|**8.500**|
|2|8.500|-0.3|6.800|**6.500**|
|3|6.500|1.0|5.200|**6.200**|
|4|6.200|-0.2|4.960|**4.760**|
|5|4.760|0.1|3.808|**3.908**|

#### Key Observations
#### Pattern Analysis:
12. **Mean Reversion**: Without error terms, the series would decay toward 0 (since 0.8 < 1)
13. **Shock Impact**: Error terms create deviations from the smooth decay pattern
14. **Persistence**: Each value still carries 80% of the previous period's influence

#### Why AR(1) is Useful:
- **Economic Modeling**: Many economic variables (GDP, inflation) show persistence
- **Financial Analysis**: Stock prices, interest rates often follow AR patterns
- **Forecasting**: Provides a simple way to model time-dependent data
- **Stationarity**: When |œÜ‚ÇÅ| < 1 (like our 0.8), the series is stationary

#### Final Answer:
- **Y‚ÇÅ = 8.5**
- **Y‚ÇÇ = 6.5**
- **Y‚ÇÉ = 6.2**
- **Y‚ÇÑ = 4.76**
- **Y‚ÇÖ = 3.908**

---

